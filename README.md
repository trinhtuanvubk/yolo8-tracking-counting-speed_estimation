# Yolo8 multi-object, segmentation and pose tracking - counting - speed estimation


<div align="center">
  <p>
  <img src="./figure/test1_yolov8n_transform3d_fix.gif" width="700"/>
  </p>
  <br>
</div>


## Repository Description

This repository is a derivative of the [mikel-brostrom/yolo_tracking](https://github.com/mikel-brostrom/yolo_tracking/tree/master) by [mikel-brostrom](https://github.com/mikel-brostrom). It builds upon their work and incorporates additional features and modifications specific to this project.


## Introduction

This repository contains a highly configurable two-stage-tracker that adjusts to different deployment scenarios. It can jointly perform multiple object tracking and instance segmentation (MOTS). The detections generated by [YOLOv8](https://github.com/ultralytics/ultralytics), a family of object detection architectures and models pretrained on the [COCO](https://arxiv.org/abs/1405.0312) dataset, are passed to the tracker of your choice. Supported ones at the moment are: [DeepOCSORT](https://arxiv.org/abs/2302.11813) [LightMBN](https://arxiv.org/pdf/2101.10774.pdf), [BoTSORT](https://arxiv.org/abs/2206.14651) [LightMBN](https://github.com/jixunbo/LightMBN)[](https://arxiv.org/pdf/2101.10774.pdf), [StrongSORT](https://github.com/dyhBUPT/StrongSORT)[](https://arxiv.org/abs/2202.13514) [LightMBN](https://github.com/jixunbo/LightMBN)[](https://arxiv.org/pdf/2101.10774.pdf), [OCSORT](https://github.com/noahcao/OC_SORT)[](https://arxiv.org/abs/2203.14360) and [ByteTrack](https://github.com/ifzhang/ByteTrack)[](https://arxiv.org/abs/2110.06864). They can track any object that your Yolov8 model was trained to detect.


## Installation
Install with pip:
```
git clone https://github.com/trinhtuanvubk/yolo8-tracking-counting-speed_estimation.git
cd yolo8-tracking-counting-speed_estimation
pip install -r requirements.txt  # install dependencies
```
Install with Docker:
```
git clone https://github.com/trinhtuanvubk/yolo8-tracking-counting-speed_estimation.git
cd yolov8-tracking-counting-speed_estimation
docker build -t <image-tag-name> .
docker run --restart always -itd -v $(pwd):/yolo8-tracking-counting-speed_estimation --name <container-name> -w/yolo8-tracking-counting-speed_estimation <image-tag-name>
```
If you get errors raised by opencv and ffmpeg, you can pull this image:
```
git clone https://github.com/trinhtuanvubk/yolo8-tracking-counting-speed_estimation.git
cd yolov8-tracking-counting-speed_estimation
docker pull trinhtuanvubk/vutt-yolo8-opencv
docker run --restart always -itd -v $(pwd):/yolo8-tracking --name <container-name> -w/yolov8-tracking-counting-speed_estimation trinhtuanvubk/vutt-yolo8-opencv:tracking
```

</details>

## Tracking, Counting, Speed Estimation

-To run: 

```
$ python3 main.py --scenario main \
--yolo-model weights/yolov8n.pt \
--tracking-method bytetrack \
--speed-method transform_3d \
--source test_data/test1.mp4 \
--classes 0 1 2 3 5 6 7 \
--name test1_line \
--save \
--save-txt \
```

- Flag:
    - `--scenario`: select method(`track`, `eval`, `evolve`). Just support track now
    - `--yolo-model`: select model (`yolov8n`, `yolo_nas_n`, `yolox_n`, `yolov8n-seg`, `yolov8n-pose`) or other yolo models.
    - `--tracking-method`: select tracking method (`bytetrack`, `deepocsort`, `strongsort`, `ocsort`, `botsort`  )
    - `--speed-method`: select speed estimation method (`twopoints`, `twolines`, `birdeyes`, `transform_3d`)
    - `--source`: select source type (`0`, `img.jpg`, `vid.mp4`, `path/`, `path/*.jpg`, `'https://youtu.be/Zgi9g1ksQHc'`, `'rtsp://example.com/media.mp4'`) corresponding to webcam, image, video, directory, glob, youtube, rtsp-rtmp-http stream
    - `--class`: select subset of classes corresponding index of classes in COCO dataset
    - `--name`: the ouput directory name 
    - `--save`: flag to save output image/video
    - `--save-txt`: flag to write the result as .txt file